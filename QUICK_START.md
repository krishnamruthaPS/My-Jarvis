
# âš¡ QUICK REFERENCE

## ðŸš€ STARTUP (Every Time)

### Terminal 1 - Ollama
```powershell
$env:OLLAMA_NUM_GPU = "0"
ollama serve
```

### Terminal 2 - Flask
```powershell
$env:OLLAMA_NUM_GPU = "0"
cd C:\Users\KAVITHA\OneDrive\Desktop\san\vexa
python app.py
```

### Browser
```
http://localhost:5000
```

---

## ðŸ“– ONE MINUTE EXPLANATION

**When you ask "Who is Raju?":**

1. **Convert to numbers:** Question becomes 768 numbers
2. **Search database:** Find most similar document chunks
3. **Get context:** Retrieve 5 most relevant chunks
4. **Ask LLM:** "Read this context, who is Raju?"
5. **Get answer:** LLM generates response
6. **Display:** Answer appears in browser

**Result:** Accurate, context-based answer! âœ…

---

## ðŸ“š SIMPLE EXAMPLE

### Upload Story about Raju
```
Action: Click "Ingest Documents"
Input:  C:\Users\KAVITHA\OneDrive\Desktop\san\story\
Result: âœ… Successfully ingested 2 chunks!
```

### Ask Question
```
Action: Type question
Input:  "Who is Raju?"
Result: "Raju is a clever little rabbit..."
```

---

## ðŸŽ¯ TECHNOLOGY STACK

| Layer | Technology | Location |
|-------|-----------|----------|
| **LLM** | Ollama tinyllama | Local (CPU) |
| **Embeddings** | Ollama nomic-embed-text | Local (CPU) |
| **Database** | Pinecone | Cloud |
| **Web Server** | Flask | Local |
| **UI** | HTML + JavaScript | Browser |

---

## âœ… SYSTEM STATUS

- âœ… Ollama running in CPU mode
- âœ… Embeddings working (Ollama)
- âœ… Pinecone connected
- âœ… Flask serving web UI
- âœ… Documents ingesting
- âœ… Questions answering

---

## ðŸ†˜ QUICK FIXES

**"Connection Failed"**
â†’ Make sure Ollama (Terminal 1) is running first

**"Model memory error"**
â†’ Already using tinyllama (smallest)
â†’ Set `$env:OLLAMA_NUM_GPU = "0"` in Terminal 1

**"No documents found"**
â†’ Ingest documents first using web UI
â†’ Then ask questions

---

## ðŸ“Š PERFORMANCE

- **Upload docs:** 10-30 sec
- **Per query:** 30-60 sec (CPU mode)
- **Database search:** <100ms
- **Model loading:** ~10 sec

---

## ðŸ—‚ï¸ PROJECT FILES (11 TOTAL)

```
.venv/                  Virtual environment
.env                    Configuration
app.py                  Flask server (main)
config.py               Load variables
embeddings.py           Ollama embeddings
ingestor_simple.py      Upload documents
query_simple.py         Answer questions
requirements.txt        Dependencies
templates/index.html    Web UI
README.md               Full docs
HOW_IT_WORKS.md         Detailed explanation
```

---

## ðŸ’¡ HOW EMBEDDINGS HELP

**Without:** Only finds exact word matches
**With:** Understands meaning and context

Example:
- Q: "Who is Raju?"
- Without embeddings: Only finds documents with word "Raju"
- With embeddings: Also finds "the rabbit", "clever animal", etc.

---

## ðŸ”§ CONFIGURE

Edit `.env`:
```env
PINECONE_API_KEY=your_key_here
PINECONE_ENV=us-east-1
PINECONE_INDEX_NAME=vexa
VECTOR_DIM=768
LLM_MODEL=tinyllama
```

---

## ðŸ“ SUPPORTED FILE TYPES

âœ… PDF (.pdf)
âœ… Text (.txt)
âœ… Markdown (.md)

---

## ðŸŽ¯ 3-STEP PROCESS

```
1. UPLOAD DOCUMENTS
   Folder â†’ Split â†’ Embed â†’ Store

2. ASK QUESTION
   Question â†’ Embed â†’ Search â†’ Retrieve

3. GET ANSWER
   Context â†’ LLM â†’ Generate â†’ Display
```

---

## âœ¨ KEY FEATURES

- âœ… **Local:** Everything runs on your machine
- âœ… **Private:** No external APIs (except Pinecone cloud)
- âœ… **Free:** No OpenAI/paid APIs
- âœ… **Fast:** Optimized for CPU
- âœ… **Smart:** Semantic search understands meaning
- âœ… **Simple:** Clean, minimal codebase

---

**Ready to use! Open http://localhost:5000** ðŸš€

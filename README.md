# ğŸ” Vexa - Local QA System

A **completely local** question-answering system that ingests documents and answers questions using:
- **Ollama** (local LLM - tinyllama)
- **Pinecone** (vector database - cloud storage for embeddings)
- **Semantic Embeddings** (Ollama's nomic-embed-text - local)
- **Flask** (web interface)

**âœ… NO OpenAI API, NO external models - Everything runs on YOUR machine!**

---

## ğŸ¯ How It Works (Simple Explanation)

### **Step 1: You Upload Documents** ğŸ“„
```
Your Files (PDF, TXT, MD)
    â†“
Read and extract text
    â†“
Split into small chunks (500 characters each)
    â†“
Convert to embeddings (numbers representing meaning)
    â†“
Store in Pinecone (cloud vector database)
```

### **Step 2: You Ask a Question** â“
```
Your Question: "Who is Raju?"
    â†“
Convert question to embedding (same way as documents)
    â†“
Search Pinecone: "Which document chunks are most similar?"
    â†“
Get back Top 5 Most Relevant Chunks
    â†“
Send Chunks + Question to Local LLM (tinyllama)
    â†“
LLM reads context and generates answer
    â†“
You see the answer! âœ…
```

### **Why This Works**
- **Embeddings** capture the *meaning* of text (not just keywords)
- **Semantic search** finds relevant information even if words don't match exactly
- **Local LLM** reads the context and understands the question
- **Result:** Accurate, context-based answers!

---

## ğŸ“ Project Files

```
vexa/
â”œâ”€â”€ .venv/                # Python virtual environment
â”œâ”€â”€ .env                  # Configuration (API keys, model selection)
â”œâ”€â”€ app.py                # Flask web server - serves the UI
â”œâ”€â”€ config.py             # Load environment variables
â”œâ”€â”€ embeddings.py         # Semantic embeddings (Ollama nomic-embed-text)
â”œâ”€â”€ ingestor_simple.py    # Upload & process documents
â”œâ”€â”€ query_simple.py       # Answer questions
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Web interface (HTML + JavaScript)
â””â”€â”€ README.md             # This file!
```

---

## ğŸš€ Quick Start

### **Prerequisites**
- Python 3.10+
- Ollama installed (https://ollama.ai)
- Pinecone API key (free: https://www.pinecone.io)

### **Setup (Windows PowerShell)**

```powershell
# 1. Activate virtual environment
.\.venv\Scripts\Activate.ps1

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download Ollama embedding model
ollama pull nomic-embed-text
```

### **Configure .env**

Edit the `.env` file with your settings:

```env
PINECONE_API_KEY=your_api_key_here
PINECONE_ENV=us-east-1
PINECONE_INDEX_NAME=vexa
VECTOR_DIM=768
LLM_MODEL=tinyllama
```

### **Run the System**

**Terminal 1 - Start Ollama (CPU mode - no GPU needed)**
```powershell
$env:OLLAMA_NUM_GPU = "0"
ollama serve
```

**Terminal 2 - Start Flask**
```powershell
$env:OLLAMA_NUM_GPU = "0"
python app.py
```

**Open Browser**
```
http://localhost:5000
```

---

## ğŸ“ How to Use

### **1. Ingest (Upload) Documents**
1. Click **Ingest Documents** section
2. Enter folder path: `C:\Users\KAVITHA\OneDrive\Desktop\san\story\`
3. Click **Ingest** button
4. Wait for "âœ… Successfully ingested X chunks!" message

**Supported file types:** .txt, .md, .pdf

### **2. Ask Questions**
1. Click **Query Documents** section
2. Type your question: "Who is Raju?"
3. Click **Search** button
4. Wait for answer (30-60 seconds on CPU mode)
5. Read your answer!

**Example questions:**
- "Who is Raju?"
- "What does Raju do?"
- "Describe Raju's characteristics"
- "What happened on a sunny day?"

---

## ğŸ”¬ Technical Architecture

### **Embeddings (Text â†’ Numbers)**
- **Model:** Ollama nomic-embed-text
- **Dimensions:** 768 (each text becomes 768 numbers)
- **Location:** Runs locally on your machine
- **Speed:** Instant
- **Purpose:** Convert text into "meaning" that computers can compare

### **Vector Database (Search)**
- **Service:** Pinecone (cloud)
- **Why cloud?** Reliable, fast vector search engine
- **What's stored?** Only embeddings (numbers), not full text
- **Index:** "vexa" with cosine similarity metric
- **Privacy:** Embeddings are just math, not your actual text

### **Language Model (LLM)**
- **Model:** tinyllama (637 MB)
- **Location:** Ollama (local)
- **Mode:** CPU-only (no GPU needed)
- **Speed:** 30-60 seconds per answer
- **Purpose:** Read context and generate natural language answers

### **Web Server**
- **Framework:** Flask
- **Frontend:** HTML + JavaScript (AJAX)
- **Interface:** Beautiful, responsive, real-time updates

---

## ğŸ“Š Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Documents  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (ingestor_simple.py)
         â”œâ”€ Read files
         â”œâ”€ Split into chunks
         â””â”€ Generate embeddings (Ollama)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Pinecone   â”‚ â—„â”€â”€â”€ Cloud Vector DB
    â”‚  (stores     â”‚      (Search + retrieve)
    â”‚ embeddings)  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ (query_simple.py)
          â”œâ”€ Embed user question (Ollama)
          â”œâ”€ Search Pinecone
          â””â”€ Get top 5 most similar chunks
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama LLM      â”‚
    â”‚ (tinyllama)      â”‚â—„â”€â”€â”€ Local Language Model
    â”‚ Generate answer  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Browser    â”‚ â—„â”€â”€â”€ You see the answer!
    â”‚   (http://   â”‚
    â”‚ localhost:   â”‚
    â”‚   5000)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Example Walkthrough

### **Scenario: Ingesting a Story**

```
Action: Click Ingest, enter "C:\Users\KAVITHA\OneDrive\Desktop\san\story\"
         
Processing:
  1. Read file: rabit.pdf
  2. Extract text: "Once upon a time, in a green forest..."
  3. Split into chunks (each ~500 chars)
  4. Generate embeddings for each chunk (using Ollama nomic-embed-text)
  5. Upload to Pinecone

Result: âœ… Successfully ingested 2 chunks!
```

### **Scenario: Asking a Question**

```
Action: Type "Who is Raju?" and click Search

Processing:
  1. Embed question: "Who is Raju?" â†’ [0.45, -0.23, 0.89, ...]
  2. Search Pinecone: "Find most similar chunks"
  3. Retrieve chunks:
     Chunk 1: "Once upon a time, in a green forest, there lived a 
              little rabbit named Raju..."
     Chunk 2: "From that day on, the fox never tried to chase Raju 
              again, and Raju became known as the clever little rabbit 
              of the forest."
  4. Create prompt:
     ---
     You are helpful assistant. Answer ONLY using context below.
     
     CONTEXT:
     [chunks from Pinecone]
     
     QUESTION: Who is Raju?
     
     ANSWER:
     ---
  5. Send to Ollama LLM (tinyllama)
  6. LLM thinks and writes answer:
     "Raju is a clever little rabbit who lived in a green forest. 
      He is known for being small but very smart and careful..."

Result: Answer displayed in browser! âœ…
```

---

## âš™ï¸ System Configuration

### **.env File Explanation**

```env
# Your Pinecone API key (from https://www.pinecone.io)
PINECONE_API_KEY=pcsk_xxxx...

# Pinecone region
PINECONE_ENV=us-east-1

# Name of your index (where embeddings are stored)
PINECONE_INDEX_NAME=vexa

# Embedding dimension (must match index)
VECTOR_DIM=768

# Ollama model to use (tinyllama = small, fast, CPU-friendly)
LLM_MODEL=tinyllama
```

### **Ollama Environment Variable**

```powershell
# This tells Ollama to use CPU instead of GPU
# Needed if you don't have GPU or want CPU-only mode
$env:OLLAMA_NUM_GPU = "0"

# To use GPU (if available):
# Remove this line or set to "1"
```

---

## ğŸ¯ Why This Design

| Component | Why? | Benefit |
|-----------|------|---------|
| **Ollama** | Open source, runs locally | No API costs, completely private |
| **nomic-embed-text** | Best local embeddings | Semantic search works great |
| **tinyllama** | Tiny but capable | Works on CPU, no GPU needed |
| **Pinecone** | Fast vector search | Reliable, scalable retrieval |
| **Flask** | Lightweight | Simple, no heavy dependencies |

---

## âš¡ Performance Notes

- **First query:** 30-60 seconds (model loads + generates)
- **Subsequent queries:** 30-40 seconds (CPU mode)
- **GPU mode:** Would be 5-10x faster (if GPU available)
- **Embedding speed:** <1 second per chunk
- **Search speed:** <100ms

**To speed up (if GPU available):**
- Remove `$env:OLLAMA_NUM_GPU = "0"` before running
- System will automatically use GPU for faster answers

---

## ğŸ” Privacy & Security

âœ… **Your data stays with you:**
- Embeddings generated locally (Ollama)
- LLM runs locally (tinyllama)
- Only embeddings sent to Pinecone (just numbers, not text)
- No OpenAI, no external APIs, no tracking

---

## ğŸ†˜ Troubleshooting

### **Error: "Model requires more system memory"**
- **Fix:** Already set to use tinyllama (smallest model)
- **Check:** Make sure `OLLAMA_NUM_GPU = "0"` is set before running
- **Solution:** Restart both terminals with CPU mode enabled

### **Error: "No relevant documents found"**
- **Fix:** Check you actually ingested documents
- **Solution:** Try asking different questions
- **Check:** Go to Pinecone dashboard and verify index has vectors

### **Error: "Ollama connection failed"**
- **Fix:** Ollama not running in first terminal
- **Solution:** Start Terminal 1 first: `ollama serve`
- **Wait:** Give it 10 seconds to start

### **Flask page shows "Error" when querying**
- **Check:** Is both Ollama terminal and Flask terminal still running?
- **Fix:** Restart both in correct order (Ollama first, then Flask)

---

## ğŸ“¦ Dependencies (What Gets Installed)

```
flask              # Web server framework
python-dotenv      # Load .env configuration
pinecone==7.3.0    # Vector database client
ollama             # Ollama integration
PyPDF2             # Read PDF files
```

Install all with:
```powershell
pip install -r requirements.txt
```

---

## ğŸ“ Understanding Embeddings

### **What are Embeddings?**

Embeddings are numbers that represent the *meaning* of text.

**Without embeddings (keyword search):**
```
Q: "Who is Raju?"
Search: Find documents containing "Raju"
Result: Only exact matches
Problem: âŒ Misses "the rabbit" or "the character"
```

**With embeddings (semantic search):**
```
Q: "Who is Raju?" â†’ [0.45, -0.23, 0.89, ...] (768 numbers)
Document: "There lived a rabbit named Raju" â†’ [0.42, -0.21, 0.91, ...]

Computer calculates: These are very similar! (cosine similarity = 0.92)
Result: Returns this document
âœ… Understands meaning, not just keywords!
```

---

## âœ… System Status

Everything is working perfectly! âœ¨

- âœ… **Ollama LLM** (tinyllama, 637MB, CPU-friendly)
- âœ… **Embeddings** (nomic-embed-text, local, instant)
- âœ… **Pinecone** (768-dim index, fast search)
- âœ… **Flask Web Server** (responsive, real-time)
- âœ… **Document Ingestion** (PDF, TXT, MD support)
- âœ… **Question Answering** (context-based, accurate)
- âœ… **Beautiful UI** (modern, intuitive interface)

---

## ğŸ‰ You're Ready!

1. Open **http://localhost:5000**
2. Upload your documents
3. Ask questions
4. Get accurate, context-based answers!

**Enjoy your local QA system! ğŸš€**

### Query Documents
1. Type your question
2. Click "Search"
3. Get answer from Ollama LLM

## Project Structure

```
vexa/
â”œâ”€â”€ app.py                 # Flask web server
â”œâ”€â”€ embeddings.py          # MockEmbeddings class
â”œâ”€â”€ ingestor_langchain.py  # Document ingestion
â”œâ”€â”€ query_langchain.py     # Document querying
â”œâ”€â”€ config.py              # Configuration loader
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # API keys & settings
â””â”€â”€ templates/
    â””â”€â”€ index.html         # Web UI
```

## Key Features

âœ… **No External Model Downloads** - Uses MockEmbeddings (instant, no dependencies)  
âœ… **Local LLM** - Ollama runs on your machine (tinyllama: 637MB)  
âœ… **No API Calls** - Everything is local, no rate limits  
âœ… **Simple Web UI** - Ingest and query documents easily  

## System Architecture

```
Document Folder
    â†“
ingestor_langchain.py
    â”œâ”€ Read files
    â”œâ”€ Split into chunks
    â”œâ”€ Embed with MockEmbeddings
    â””â”€ Store in Pinecone
        
User Query
    â†“
query_langchain.py
    â”œâ”€ Embed query
    â”œâ”€ Search Pinecone
    â”œâ”€ Send to Ollama LLM
    â””â”€ Return answer
```

## Dependencies

- flask
- python-dotenv
- pinecone==7.3.0
- langchain>=0.0.302,<1.0
- ollama
- PyPDF2

Install with: `pip install -r requirements.txt`

## Troubleshooting

**GPU Memory Error**: Ollama tries to use GPU. Force CPU mode:
```powershell
$env:OLLAMA_NUM_GPU = "0"
ollama serve
```

**Model Issues**: Download tinyllama:
```powershell
ollama pull tinyllama
```

## License

MIT
#   M y - J a r v i s  
 